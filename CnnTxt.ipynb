{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "from pickle import dump\n",
    "#load data\n",
    "def load_document(file_name):\n",
    "    file= open(file_name,'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "#preprocessing the data and creating tokens\n",
    "def document_to_tokens(document):\n",
    "    #split the document into words\n",
    "    tokens=document.split()\n",
    "    #remove the punctuation\n",
    "    table=str.maketrans('','',punctuation)\n",
    "    tokens=[w.translate(table) for w in tokens ]\n",
    "    #remove the tokens that are not alphabetic\n",
    "    tokens=[w for w in tokens if w.isalpha()]\n",
    "    #remove the stop words\n",
    "    stop_words=stopwords.words('english')\n",
    "    tokens=[w for w in tokens if not w in stop_words]\n",
    "    #remove the words -tokens-that has size less or equal to one\n",
    "    tokens=[w for w in tokens if len(w)>1]\n",
    "    tokens=' '.join(tokens)\n",
    "    return tokens \n",
    "#load all the documents from the hard disk\n",
    "def handle_documents(directory,is_train):\n",
    "    documents=list()\n",
    "    for file_name in listdir(directory):\n",
    "        if is_train and file_name.startswith('cv9'):\n",
    "            continue;\n",
    "\n",
    "        if not  is_train and not file_name.startswith('cv9'):\n",
    "            continue;    \n",
    "        \n",
    "        path=directory + '/' + file_name\n",
    "\n",
    "        #load the document\n",
    "        document=load_document(path)\n",
    "        #do the preprocessing\n",
    "        tokens=document_to_tokens(document)\n",
    "\n",
    "        documents.append(tokens)\n",
    "\n",
    "    return documents    \n",
    "\n",
    "def save_data(dataset,filename):\n",
    "    dump(dataset,open(filename,'wb'))\n",
    "    print('%s is saved' % filename)     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the training files\n",
    "neg_docs_train=handle_documents('C:\\\\Users\\\\yaici\\\\OneDrive\\\\Bureau\\\\CNN for Text Classification\\\\txt_sentoken\\\\neg',True)\n",
    "pos_docs_train=handle_documents('C:\\\\Users\\\\yaici\\\\OneDrive\\\\Bureau\\\\CNN for Text Classification\\\\txt_sentoken\\\\pos',True)\n",
    "\n",
    "#load the testing files\n",
    "neg_docs_test=handle_documents('C:\\\\Users\\\\yaici\\\\OneDrive\\\\Bureau\\\\CNN for Text Classification\\\\txt_sentoken\\\\neg',False)\n",
    "pos_docs_test=handle_documents('C:\\\\Users\\\\yaici\\\\OneDrive\\\\Bureau\\\\CNN for Text Classification\\\\txt_sentoken\\\\pos',False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.pk1 is saved\n",
      "test.pk1 is saved\n"
     ]
    }
   ],
   "source": [
    "trainx= neg_docs_train + pos_docs_train\n",
    "trainy=[0 for _ in range(900)] + [1 for _ in range(900)] \n",
    "\n",
    "save_data([trainx,trainy],'train.pk1')\n",
    "\n",
    "testx= neg_docs_test + pos_docs_test\n",
    "testy=[0 for _ in range(100)] + [1 for _ in range(100)] \n",
    "\n",
    "save_data([testx,testy],'test.pk1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# define a function to load the data \n",
    "def load_data(file_name):\n",
    "    return load(open(file_name,'rb'))\n",
    "\n",
    "def build_tokenizer(words):\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(words)\n",
    "    return t\n",
    "\n",
    "#calculate the maximum size\n",
    "def max_length(docs):\n",
    "    return max([len(doc.split())for doc in docs])\n",
    "\n",
    "#encode the words\n",
    "def encoding(Tokenizer,words,length):\n",
    "    encoded=Tokenizer.texts_to_sequences(words)\n",
    "    encoded_pad= pad_sequences(encoded,maxlen=length,padding='post') \n",
    "    return encoded_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum document size = 1380\n",
      "the number of vocabulary = 44277\n",
      "(1800, 1380)\n"
     ]
    }
   ],
   "source": [
    "trainx,trainy=load_data('train.pk1')\n",
    "tokenizer=build_tokenizer(trainx)\n",
    "#maximum document size\n",
    "length=max_length(trainx)\n",
    "\n",
    "# the number of vocabulary\n",
    "vocab_size = len(tokenizer.word_index) +1\n",
    "\n",
    "print('maximum document size = %d'% length)\n",
    "print('the number of vocabulary = %d'% vocab_size)\n",
    "\n",
    "train_data= encoding(tokenizer,trainx,length)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Embedding, Conv1D, MaxPooling1D, concatenate\n",
    "from tensorflow.keras.utils import plot_model\n",
    "# define the model\n",
    "def build_model(vocab_size,length):\n",
    "    #first channel\n",
    "    inputs1=Input(shape=(length,))\n",
    "    embedding1=Embedding(vocab_size,100)(inputs1)\n",
    "    conv1=Conv1D(filters=32,kernel_size=4,activation='relu')(embedding1)\n",
    "    drop1=Dropout(0.5)(conv1)\n",
    "    pool1=MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1=Flatten()(pool1)\n",
    "\n",
    "    #second channel\n",
    "    inputs2=Input(shape=(length,))\n",
    "    embedding2=Embedding(vocab_size,100)(inputs2)\n",
    "    conv2=Conv1D(filters=32,kernel_size=4,activation='relu')(embedding2)\n",
    "    drop2=Dropout(0.5)(conv2)\n",
    "    pool2=MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2=Flatten()(pool2)\n",
    "\n",
    "    #third channel\n",
    "    inputs3=Input(shape=(length,))\n",
    "    embedding3=Embedding(vocab_size,100)(inputs3)\n",
    "    conv3=Conv1D(filters=32,kernel_size=4,activation='relu')(embedding3)\n",
    "    drop3=Dropout(0.5)(conv3)\n",
    "    pool3=MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3=Flatten()(pool3)\n",
    "\n",
    "    #merging the three channels\n",
    "    channels= concatenate([flat1,flat2,flat3]) \n",
    "    \n",
    "    #pass the merged inputs\n",
    "    dense=Dense(10,activation='relu')(channels)\n",
    "    outputs=Dense(1,activation='sigmoid')(dense)\n",
    "    model=Model(inputs=[inputs1,inputs2,inputs3],outputs=outputs)\n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configure & draw & train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 168ms/step - accuracy: 0.5185 - loss: 0.7024\n",
      "Epoch 2/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 213ms/step - accuracy: 0.7359 - loss: 0.5761\n",
      "Epoch 3/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 205ms/step - accuracy: 0.9864 - loss: 0.0643\n",
      "Epoch 4/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 187ms/step - accuracy: 1.0000 - loss: 0.0055\n",
      "Epoch 5/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 196ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 6/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 189ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 7/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 188ms/step - accuracy: 1.0000 - loss: 8.1571e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 186ms/step - accuracy: 1.0000 - loss: 6.1745e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 188ms/step - accuracy: 1.0000 - loss: 4.6522e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 193ms/step - accuracy: 1.0000 - loss: 5.2913e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pydot\n",
    "import pydotplus\n",
    "from pydotplus import graphviz\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Embedding, Conv1D, MaxPooling1D, concatenate\n",
    "from tensorflow.keras.utils import plot_model\n",
    "model=build_model(vocab_size,length)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "plot_model(model,'my_mode.jpg',show_shapes=True)\n",
    "#print(model.summary())\n",
    "\n",
    "#training the model\n",
    "model.fit([train_data,train_data,train_data],array(trainy),epochs=10,batch_size=16)\n",
    "model.save('my_mode.h5')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 86.000001\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "#load the model\n",
    "\n",
    "my_model=load_model('my_mode.h5')\n",
    "testx,testy==load_data('test.pk1')\n",
    "test_data=encoding(tokenizer,testx,length)\n",
    "\n",
    "#evaluation based on the model\n",
    "loss,accuracy=model.evaluate([test_data,test_data,test_data],array(testy),verbose=0)\n",
    "print('test accuracy %f' % (accuracy*100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
